---
layout: default
title: Lab 2 - Explore Traces
nav_order: 4
---

# üîç Lab 2: Exploring AI Traces in Dynatrace

**Duration:** ~30 minutes

In this lab, you'll explore the traces generated by your AI application in Dynatrace, understanding the insights available for LLM and RAG observability.

---

## üéØ Learning Objectives

- Navigate to distributed traces in Dynatrace
- Analyze LLM call details including prompts and completions
- Understand token usage and cost attribution
- Explore RAG pipeline spans (embeddings, vector search, completion)
- Create basic queries for AI observability

---

## Step 1: Access Dynatrace

### 1.1 Open Dynatrace

Open the Dynatrace environment URL provided by your instructor:

```
https://YOUR_ENV.live.dynatrace.com
```

### 1.2 Login

Use the credentials provided by your instructor.

---

## Step 2: Find Your Service

### 2.1 Navigate to Services

1. In the left navigation menu, click **Services** (or use the search bar)
2. Click the **Explorer** tab on the top and look for your service: `ai-chat-service-{YOUR_ATTENDEE_ID}`

   For example: `ai-chat-service-jsmith`

   <img src="/assets/images/find-service.png" alt="Find Your Service" style="max-width: 100%; height: auto; max-height: 400px;" />

### 2.2 Open Service Details

Click on your service to select the **Additional Telemetry** page. You'll see:

- Request rate
- Request duration
- Tokens used
- Embeddings vector size

---

## Step 3: Explore Distributed Traces

### 3.1 Navigate to Traces

1. From your service page, click on **View traces** in the left panel
2. Or navigate via: **Distributed traces**

### 3.2 Filter for Your Service

Use the filter to show only traces from your service:

1. Click **Spans** on the left
4. Choose `ai-chat-service-{YOUR_ATTENDEE_ID}`

### 3.3 Select a Trace

Click on any trace to view the details. You should see traces for your `/chat` endpoint.

---

## Step 4: Analyze an AI Trace

### 4.1 Understanding the Trace Structure

A typical RAG request trace includes these spans:

```
üìç rag_chat_pipeline.workflow (Main RAG pipeline)
  ‚îî‚îÄ‚îÄ üìç analyze_query_intent.task (Classify user query type)
      ‚îî‚îÄ‚îÄ üìç AzureChatOpenAI.chat (LLM call for classification)
  ‚îî‚îÄ‚îÄ üìç retrieve_documents.task (Document retrieval)
      ‚îî‚îÄ‚îÄ üìç openai.embeddings (Generate query embedding)
      ‚îî‚îÄ‚îÄ üìç chroma.query (Vector store search)
  ‚îî‚îÄ‚îÄ üìç generate_context.task (Format retrieved docs)
  ‚îî‚îÄ‚îÄ üìç generate_response.task (Generate final answer)
      ‚îî‚îÄ‚îÄ üìç AzureChatOpenAI.chat (LLM completion call)
```

### 4.2 Examine the LLM Span

Click on the `azure_openai.chat` span under the `analyze_query_intent.task` to see:

| Attribute | Description |
|-----------|-------------|
| `gen_ai.system` | The LLM provider (Azure) |
| `gen_ai.request.model` | The model requested (gpt-4o-2024-11-20) |
| `gen_ai.response.model` | The model that responded |
| `gen_ai.request.temperature` | Temperature setting (e.g., 0.7) |
| `gen_ai.usage.input_tokens` | Number of input tokens |
| `gen_ai.usage.output_tokens` | Number of output tokens |
| `gen_ai.usage.cache_read_input_tokens` | Cached input tokens (prompt caching) |

### 4.3 View Prompts and Responses

> **Note:** Depending on configuration, you may see:
> - `gen_ai.prompt.0.content` - The input prompt content
> - `gen_ai.prompt.0.role` - The prompt role (user, system)
> - `gen_ai.completion.0.content` - The generated response content
> - `gen_ai.completion.0.role` - The completion role (assistant)
> - `gen_ai.completion.0.finish_reason` - Why generation stopped (stop, length)

This visibility is crucial for debugging AI applications!

---

## Step 5: Analyze Embedding Spans

### 5.1 Find the Embedding Span

In the trace view, locate the `openai.embeddings` span.

### 5.2 Examine Embedding Details

Key attributes include:

| Attribute | Description |
|-----------|-------------|
| `gen_ai.request.model` | Embedding model (text-embedding-3-large) |
| `gen_ai.usage.input_tokens` | Tokens in the text being embedded |
| `gen_ai.system` | The provider (Azure) |

---

## Step 6: Vector Store Spans

### 6.1 Find the Vector Store Span

Look for `chroma.query` or similar vector database spans.

### 6.2 Key Insights

Click on the `chroma.query` span to see database attributes:

| Attribute | Description |
|-----------|-------------|
| `db.system` | The vector database (chroma) |
| `db.operation` | The operation performed (query) |
| `db.chroma.query.n_results` | Number of documents retrieved (e.g., 3) |
| `db.chroma.query.embeddings_count` | Number of embeddings in the query (e.g., 1) |

---

## Step 7: Using Notebooks for AI Analysis

Dynatrace Notebooks provide powerful querying capabilities for AI observability.

### 7.1 Create a New Notebook

1. Navigate to **Notebooks** in the left-hand menu
2. Click **+ Notebook** on the top to create a new notebook
3. Name it: `AI Observability - {YOUR_ATTENDEE_ID}`

### 7.2 Query: Token Usage Over Time

Add a new section and enter this DQL query:

```sql
fetch spans
| filter service.name == "ai-chat-service-{YOUR_ATTENDEE_ID}"
| filter isNotNull(gen_ai.usage.input_tokens)
| makeTimeseries total_input_tokens = sum(gen_ai.usage.input_tokens),
    total_output_tokens = sum(gen_ai.usage.output_tokens),
    request_count = count()
```

### 7.3 Query: Model Usage Distribution

```sql
fetch spans
| filter service.name == "ai-chat-service-{YOUR_ATTENDEE_ID}"
| filter isNotNull(gen_ai.request.model)
| summarize request_count = count(), by: {gen_ai.request.model}
| sort request_count desc
```

### 7.4 Query: Average Response Time by Operation

```sql
fetch spans
| filter service.name == "ai-chat-service-{YOUR_ATTENDEE_ID}"
| summarize 
    avg_duration = avg(duration),
  by: {span.name}
| sort avg_duration desc
```

---

## Step 8: Create a Dashboard

### 8.1 Create Dashboard

1. Navigate to **Dashboards** in the left-hand menu
2. Click **+ Dashboard** on the top to create a new dashboard
3. Name it: `AI Service Monitoring - {YOUR_ATTENDEE_ID}`

### 8.2 Add Tiles

Add DQL tiles for:

1. **Token Usage** - Line chart of tokens over time
2. **Request Rate** - Requests per minute
3. **Response Time** - Average and P95 latency
4. **Error Rate** - Failed requests percentage

### 8.3 Example Tile: Token Cost Estimation

Create a custom metric for cost (assuming GPT-4o pricing):

```sql
fetch spans
| filter service.name == "ai-chat-service-{YOUR_ATTENDEE_ID}"
| filter isNotNull(gen_ai.usage.input_tokens)
| makeTimeseries total_input_tokens = sum(gen_ai.usage.input_tokens),
    total_output_tokens = sum(gen_ai.usage.output_tokens)
| fieldsAdd estimated_cost_usd = (arraySum(total_input_tokens) * 0.0025 + arraySum(total_output_tokens) * 0.01) / 1000
```

---

## üî¨ Additional Hands-On Exercises

Complete these exercises to deepen your understanding:

### Exercise 1: Find the Slowest Request

Use the trace list to identify the request with the highest response time. What caused it?

### Exercise 2: Token Analysis

Calculate the average tokens per request. How many tokens do embedding vs. completion calls use?

### Exercise 3: Compare RAG vs Direct

Compare traces where `use_rag=true` vs `use_rag=false`. What's the performance difference?

---

## ‚úÖ Checkpoint

Before proceeding to Lab 3, verify you can:

- [ ] Find your service in Dynatrace
- [ ] View distributed traces for your AI requests
- [ ] Identify LLM spans and their attributes
- [ ] See token usage metrics
- [ ] Create basic DQL queries for AI observability
- [ ] Understand the trace structure (HTTP ‚Üí Embedding ‚Üí Vector ‚Üí LLM)

---

## üÜò Troubleshooting

### "No traces found"

1. Verify your service name matches your `ATTENDEE_ID`
2. Wait 1-2 minutes for traces to appear
3. Check that your application is running and receiving requests
4. Verify the DT_ENDPOINT and DT_API_TOKEN are correct

### "Missing LLM attributes"

1. Ensure you're using the traceloop-sdk
2. Some attributes may require specific Traceloop configuration
3. Check the span details for any available attributes

### "Service not appearing"

1. Send a few more requests to your application
2. Refresh the Dynatrace UI
3. Use search (Cmd/Ctrl + K) to find your service

---

## üéâ Great Progress!

You've explored AI traces in Dynatrace and understand how to analyze LLM observability data. Now let's learn how to use Dynatrace MCP for agentic AI interactions!

<div class="lab-nav">
  <a href="lab1-instrumentation">‚Üê Lab 1: Instrumentation</a>
  <a href="lab3-dynatrace-mcp">Lab 3: Dynatrace MCP ‚Üí</a>
</div>
