---
layout: default
title: Lab 2 - Explore Traces
nav_order: 4
---

# üîç Lab 2: Exploring AI Traces in Dynatrace

**Duration:** ~45 minutes

In this lab, you'll explore the traces generated by your AI application in Dynatrace, understanding the insights available for LLM and RAG observability.

---

## üéØ Learning Objectives

- Navigate to distributed traces in Dynatrace
- Analyze LLM call details including prompts and completions
- Understand token usage and cost attribution
- Explore RAG pipeline spans (embeddings, vector search, completion)
- Create basic queries for AI observability

---

## Step 1: Access Dynatrace

### 1.1 Open Dynatrace

Open the Dynatrace environment URL provided by your instructor:

```
https://YOUR_ENV.live.dynatrace.com
```

### 1.2 Login

Use the credentials provided by your instructor.

---

## Step 2: Find Your Service

### 2.1 Navigate to Services

1. In the left navigation menu, click **Services** (or use the search bar)
2. Look for your service: `ai-chat-service-{YOUR_ATTENDEE_ID}`

   For example: `ai-chat-service-jsmith`

   ![Find Your Service](/assets/images/find-service.png)

### 2.2 Open Service Details

Click on your service to open the service details page. You'll see:

- Request rate
- Response time
- Failure rate
- Dependencies

---

## Step 3: Explore Distributed Traces

### 3.1 Navigate to Traces

1. From your service page, click on **Distributed traces** in the left panel
2. Or navigate via: **Observe & Explore > Distributed traces**

### 3.2 Filter for Your Service

Use the filter to show only traces from your service:

1. Click **Add filter**
2. Select **Service name**
3. Choose `ai-chat-service-{YOUR_ATTENDEE_ID}`

### 3.3 Select a Trace

Click on any trace to view the details. You should see traces for your `/chat` endpoint.

---

## Step 4: Analyze an AI Trace

### 4.1 Understanding the Trace Structure

A typical RAG request trace includes these spans:

```
üìç POST /chat (HTTP request)
  ‚îî‚îÄ‚îÄ üìç azure_openai.embeddings (Embedding generation)
  ‚îî‚îÄ‚îÄ üìç chromadb.query (Vector store search)
  ‚îî‚îÄ‚îÄ üìç azure_openai.chat (LLM completion)
```

### 4.2 Examine the LLM Span

Click on the `azure_openai.chat` span to see:

| Attribute | Description |
|-----------|-------------|
| `llm.vendor` | The LLM provider (Azure OpenAI) |
| `llm.request.type` | Type of request (chat, completion) |
| `llm.model` | The model used (gpt-4o-mini deployment) |
| `llm.usage.prompt_tokens` | Number of input tokens |
| `llm.usage.completion_tokens` | Number of output tokens |
| `llm.usage.total_tokens` | Total tokens used |

### 4.3 View Prompts and Responses

> **Note:** Depending on configuration, you may see:
> - `llm.prompts` - The input prompt(s)
> - `llm.completions` - The generated response(s)

This visibility is crucial for debugging AI applications!

---

## Step 5: Analyze Embedding Spans

### 5.1 Find the Embedding Span

In the trace view, locate the `openai.embeddings` span.

### 5.2 Examine Embedding Details

Key attributes include:

| Attribute | Description |
|-----------|-------------|
| `llm.model` | Embedding model (text-embedding-ada-002) |
| `llm.usage.prompt_tokens` | Tokens in the text being embedded |
| `embedding.dimensions` | Vector dimensions (1536 for ada-002) |

---

## Step 6: Vector Store Spans

### 6.1 Find the Vector Store Span

Look for `chromadb.query` or similar vector database spans.

### 6.2 Key Insights

These spans show:
- Number of documents retrieved
- Query time
- Similarity scores (if available)

---

## Step 7: Using Notebooks for AI Analysis

Dynatrace Notebooks provide powerful querying capabilities for AI observability.

### 7.1 Create a New Notebook

1. Navigate to **Observe & Explore > Notebooks**
2. Click **+ New Notebook**
3. Name it: `AI Observability - {YOUR_ATTENDEE_ID}`

### 7.2 Query: Token Usage Over Time

Add a new section and enter this DQL query:

```sql
fetch spans
| filter service.name == "ai-chat-service-{YOUR_ATTENDEE_ID}"
| filter isNotNull(llm.usage.total_tokens)
| summarize 
    total_tokens = sum(llm.usage.total_tokens),
    avg_tokens = avg(llm.usage.total_tokens),
    request_count = count()
  by bin(timestamp, 5m)
| sort timestamp asc
```

### 7.3 Query: Model Usage Distribution

```sql
fetch spans
| filter service.name == "ai-chat-service-{YOUR_ATTENDEE_ID}"
| filter isNotNull(llm.model)
| summarize request_count = count() by llm.model
| sort request_count desc
```

### 7.4 Query: Average Response Time by Operation

```sql
fetch spans
| filter service.name == "ai-chat-service-{YOUR_ATTENDEE_ID}"
| summarize 
    avg_duration = avg(duration),
    p95_duration = percentile(duration, 95),
    count = count()
  by span.name
| sort avg_duration desc
```

---

## Step 8: Create a Dashboard

### 8.1 Create Dashboard

1. Navigate to **Observe & Explore > Dashboards**
2. Click **+ Dashboard**
3. Name it: `AI Service Monitoring - {YOUR_ATTENDEE_ID}`

### 8.2 Add Tiles

Add tiles for:

1. **Token Usage** - Line chart of tokens over time
2. **Request Rate** - Requests per minute
3. **Response Time** - Average and P95 latency
4. **Error Rate** - Failed requests percentage

### 8.3 Example Tile: Token Cost Estimation

Create a custom metric for cost (assuming GPT-4o-mini pricing):

```sql
fetch spans
| filter service.name == "ai-chat-service-{YOUR_ATTENDEE_ID}"
| filter isNotNull(llm.usage.total_tokens)
| summarize 
    total_tokens = sum(llm.usage.total_tokens),
    estimated_cost_usd = sum(llm.usage.total_tokens) * 0.00015 / 1000
  by bin(timestamp, 1h)
```

---

## Step 9: Explore the Service Flow

### 9.1 View Service Dependencies

1. Go back to your service page
2. Click on **Service flow** in the left panel

### 9.2 Understand the Flow

The service flow shows:
- Your `ai-chat-service` 
- Outbound calls to Azure OpenAI APIs
- Any other dependencies

This visualization helps understand your AI application's architecture.

---

## üî¨ Hands-On Exercises

Complete these exercises to deepen your understanding:

### Exercise 1: Find the Slowest Request

Use the trace list to identify the request with the highest response time. What caused it?

### Exercise 2: Token Analysis

Calculate the average tokens per request. How many tokens do embedding vs. completion calls use?

### Exercise 3: Compare RAG vs Direct

Compare traces where `use_rag=true` vs `use_rag=false`. What's the performance difference?

---

## ‚úÖ Checkpoint

Before proceeding to Lab 3, verify you can:

- [ ] Find your service in Dynatrace
- [ ] View distributed traces for your AI requests
- [ ] Identify LLM spans and their attributes
- [ ] See token usage metrics
- [ ] Create basic DQL queries for AI observability
- [ ] Understand the trace structure (HTTP ‚Üí Embedding ‚Üí Vector ‚Üí LLM)

---

## üÜò Troubleshooting

### "No traces found"

1. Verify your service name matches your `ATTENDEE_ID`
2. Wait 1-2 minutes for traces to appear
3. Check that your application is running and receiving requests
4. Verify the DT_ENDPOINT and DT_API_TOKEN are correct

### "Missing LLM attributes"

1. Ensure you're using the traceloop-sdk
2. Some attributes may require specific Traceloop configuration
3. Check the span details for any available attributes

### "Service not appearing"

1. Send a few more requests to your application
2. Refresh the Dynatrace UI
3. Use search (Cmd/Ctrl + K) to find your service

---

## üéâ Great Progress!

You've explored AI traces in Dynatrace and understand how to analyze LLM observability data. Now let's learn how to use Dynatrace MCP for agentic AI interactions!

<div class="lab-nav">
  <a href="lab1-instrumentation">‚Üê Lab 1: Instrumentation</a>
  <a href="lab3-dynatrace-mcp">Lab 3: Dynatrace MCP ‚Üí</a>
</div>
